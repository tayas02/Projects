{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# **Installing Required Dependencies**"
      ],
      "metadata": {
        "id": "bq_6f1rQEdIJ"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pPxyepYHD8IA"
      },
      "outputs": [],
      "source": [
        "%pip install -q pdfplumber sentence-transformers torch tqdm pyngrok streamlit datasets groq\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Configuring Groq Cloud to Load the Llama 3.2**"
      ],
      "metadata": {
        "id": "zpAD4O1zEn7t"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from groq import Groq\n",
        "client = Groq(api_key='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')"
      ],
      "metadata": {
        "id": "CJU-cOvFEI66"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Importing Require Libraries and Model**"
      ],
      "metadata": {
        "id": "VOzTVwIqE0Uo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import DataLoader\n",
        "from sentence_transformers import SentenceTransformer, InputExample\n",
        "from tqdm import tqdm\n",
        "import pdfplumber\n",
        "from google.colab import files\n",
        "import numpy as np\n",
        "from transformers import pipeline , AutoTokenizer, AutoModelForCausalLM\n",
        "\n",
        "\n",
        "\n",
        "model = SentenceTransformer(\"abhinand/MedEmbed-large-v0.1\")\n"
      ],
      "metadata": {
        "id": "XcUfCpEdEJTJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Configuring Hugging Face Token**"
      ],
      "metadata": {
        "id": "8DlQmDiNE-K-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from huggingface_hub import login\n",
        "import os\n",
        "os.environ[\"HF_TOKEN\"] = \"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\"\n",
        "login(token=os.environ['HF_TOKEN'])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "g7fJCB1REMFi",
        "outputId": "24ae6804-e904-4f87-88c2-070ba901115a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Note: Environment variable`HF_TOKEN` is set and is the current active token independently from the token you've just configured.\n",
            "WARNING:huggingface_hub._login:Note: Environment variable`HF_TOKEN` is set and is the current active token independently from the token you've just configured.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Source Code**"
      ],
      "metadata": {
        "id": "jFU-b6zUFFa4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile streamlit_app.py\n",
        "import streamlit as st\n",
        "import pdfplumber\n",
        "import numpy as np\n",
        "import torch\n",
        "from sentence_transformers import SentenceTransformer\n",
        "from sentence_transformers.losses import CoSENTLoss, MatryoshkaLoss\n",
        "from io import BytesIO\n",
        "\n",
        "#CSS for Spotify theme\n",
        "st.markdown(\"\"\"\n",
        "    <style>\n",
        "        body {\n",
        "            background-color: #191414;  /* Black background */\n",
        "            color: white;  /* White text */\n",
        "        }\n",
        "        .stButton button {\n",
        "            background-color: #1DB954;  /* Spotify green */\n",
        "            color: white;\n",
        "            border: none;\n",
        "            padding: 12px 24px;\n",
        "            border-radius: 5px;\n",
        "            font-weight: bold;\n",
        "            font-size: 16px;\n",
        "            cursor: pointer;\n",
        "        }\n",
        "        .stButton button:hover {\n",
        "            background-color: #1ed760;  /* Slightly lighter green on hover */\n",
        "        }\n",
        "        .stTextInput input, .stSelectbox select, .stFileUploader input {\n",
        "            background-color: #191414;  /* Dark background for input fields */\n",
        "            color: white;  /* White text inside inputs */\n",
        "            border: 1px solid #1DB954;  /* Green border */\n",
        "        }\n",
        "        .stTextInput input:focus, .stSelectbox select:focus, .stFileUploader input:focus {\n",
        "            border-color: #1ed760;  /* Light green on focus */\n",
        "        }\n",
        "        .sidebar .sidebar-content {\n",
        "            background-color: #191414;  /* Dark sidebar */\n",
        "            color: white;  /* White text in sidebar */\n",
        "        }\n",
        "        .css-ffhzg2 {\n",
        "            background-color: #191414;  /* Background for markdown */\n",
        "            color: white;\n",
        "        }\n",
        "        .stFileUploader {\n",
        "            background-color: #191414;  /* Dark background for file uploader */\n",
        "            color: white;  /* White text */\n",
        "        }\n",
        "        .stFileUploader input {\n",
        "            color: white;\n",
        "        }\n",
        "        h1 {\n",
        "            color: #1DB954;  /* Spotify green color for the title */\n",
        "        }\n",
        "    </style>\n",
        "\"\"\", unsafe_allow_html=True)\n",
        "\n",
        "# PDF Parsing\n",
        "def parse_pdf(uploaded_file):\n",
        "    try:\n",
        "        with pdfplumber.open(BytesIO(uploaded_file.read())) as pdf:\n",
        "            text = \"\"\n",
        "            for page in pdf.pages:\n",
        "                text += page.extract_text()\n",
        "        return text\n",
        "    except Exception as e:\n",
        "        st.error(f\"Error reading the PDF: {e}\")\n",
        "        return \"\"\n",
        "\n",
        "# Preprocess PDF Text\n",
        "def preprocess_pdf_text(extracted_text, chunk_size=5):\n",
        "    lines = extracted_text.split(\"\\n\")\n",
        "    candidates = [\n",
        "        \" \".join(lines[i : i + chunk_size]).strip()\n",
        "        for i in range(0, len(lines), chunk_size)\n",
        "    ]\n",
        "    return [c for c in candidates if c]\n",
        "\n",
        "# Truncate text for concise display\n",
        "def truncate_text(text, max_length=250):\n",
        "    \"\"\"Truncate text to a specified maximum length with ellipsis.\"\"\"\n",
        "    return text if len(text) <= max_length else text[:max_length].strip() + \"...\"\n",
        "\n",
        "# Define the model and loss\n",
        "def initialize_model_with_loss(selected_model_path):\n",
        "    matryoshka_dims = [768, 512, 256, 128, 64]\n",
        "    model = SentenceTransformer(selected_model_path)\n",
        "    base_loss = CoSENTLoss(model=model)\n",
        "    matryoshka_loss = MatryoshkaLoss(\n",
        "        model=model,\n",
        "        loss=base_loss,\n",
        "        matryoshka_dims=matryoshka_dims,\n",
        "    )\n",
        "    return model, matryoshka_loss\n",
        "\n",
        "#Used to rule out similarites between each generated output\n",
        "def rescale_similarity_to_100(similarity_matrix):\n",
        "    min_sim = np.min(similarity_matrix)\n",
        "    max_sim = np.max(similarity_matrix)\n",
        "    return ((similarity_matrix - min_sim) / (max_sim - min_sim)) * 100\n",
        "\n",
        "# Calculate Metrics\n",
        "def calculate_metrics(similarity_matrix, ground_truth, top_k=10):\n",
        "    mrr, ndcg, recall = 0.0, 0.0, 0.0\n",
        "\n",
        "    for idx, query_similarities in enumerate(similarity_matrix):\n",
        "        ranked_indices = np.argsort(query_similarities)[::-1]\n",
        "        relevant_docs = ground_truth[idx]\n",
        "\n",
        "        if not relevant_docs:\n",
        "            continue\n",
        "\n",
        "        # Mean Reciprocal Rank (MRR)\n",
        "        for rank, doc_idx in enumerate(ranked_indices):\n",
        "            if doc_idx in relevant_docs:\n",
        "                mrr += 1 / (rank + 1)\n",
        "                break\n",
        "\n",
        "        # Normalized Discounted Cumulative Gain (NDCG)\n",
        "        dcg = 0.0\n",
        "        idcg = 0.0\n",
        "        for rank, doc_idx in enumerate(ranked_indices[:top_k]):\n",
        "            if doc_idx in relevant_docs:\n",
        "                dcg += 1 / np.log2(rank + 2)\n",
        "        for rank in range(min(len(relevant_docs), top_k)):\n",
        "            idcg += 1 / np.log2(rank + 2)\n",
        "        ndcg += (dcg / idcg) if idcg > 0 else 0\n",
        "\n",
        "        # Recall\n",
        "        retrieved_relevant = len(set(ranked_indices[:top_k]) & set(relevant_docs))\n",
        "        recall += retrieved_relevant / len(relevant_docs)\n",
        "\n",
        "    num_queries = len(ground_truth)\n",
        "    return mrr / num_queries, ndcg / num_queries, recall / num_queries\n",
        "\n",
        "# Generate ground truth based on similarity threshold\n",
        "def generate_ground_truth(query_embeddings, candidate_embeddings, threshold_percentile=80, top_k=10):\n",
        "    similarity_matrix = torch.mm(query_embeddings, candidate_embeddings.T).cpu().numpy()\n",
        "    similarity_threshold = np.percentile(similarity_matrix.flatten(), threshold_percentile)\n",
        "\n",
        "    ground_truth = []\n",
        "    for query_similarities in similarity_matrix:\n",
        "        relevant_docs = [idx for idx, sim in enumerate(query_similarities) if sim >= similarity_threshold]\n",
        "        ground_truth.append(relevant_docs)\n",
        "\n",
        "    return ground_truth, similarity_matrix\n",
        "\n",
        "# Main Workflow\n",
        "st.title(\"Medical Document Retrieval with Model Selection\")\n",
        "\n",
        "# Dropdown for Model Selection\n",
        "model_options = {\n",
        "    \"MedEmbed\": \"abhinand/MedEmbed-large-v0.1\",\n",
        "    \"AllMiniLMv6\": \"sentence-transformers/all-MiniLM-L6-v2\",\n",
        "    \"BioClinical\": \"emilyalsentzer/Bio_ClinicalBERT\",\n",
        "    \"PubMed\": \"pritamdeka/S-PubMedBert-MS-MARCO\",\n",
        "}\n",
        "selected_model_name = st.selectbox(\"Select a model:\", list(model_options.keys()))\n",
        "selected_model_path = model_options[selected_model_name]\n",
        "\n",
        "uploaded_file = st.file_uploader(\"Upload a PDF file\", type=[\"pdf\"])\n",
        "if uploaded_file is not None:\n",
        "    extracted_text = parse_pdf(uploaded_file)\n",
        "    if extracted_text:\n",
        "        st.success(\"PDF Text Extracted Successfully!\")\n",
        "        candidates = preprocess_pdf_text(extracted_text)\n",
        "\n",
        "        query = st.text_input(\"Enter your query:\")\n",
        "        if query:\n",
        "            # Initialize model with Matryoshka loss\n",
        "            model, matryoshka_loss = initialize_model_with_loss(selected_model_path)\n",
        "\n",
        "            # Encode the query and candidates\n",
        "            query_embeddings = model.encode([query], convert_to_tensor=True)\n",
        "            candidate_embeddings = model.encode(candidates, convert_to_tensor=True)\n",
        "\n",
        "            # Generate similarity matrix and ground truth\n",
        "            ground_truth, similarity_matrix = generate_ground_truth(query_embeddings, candidate_embeddings)\n",
        "\n",
        "            # Rescale the similarity matrix to 0-100 range\n",
        "            similarity_matrix_rescaled = rescale_similarity_to_100(similarity_matrix)\n",
        "\n",
        "            # Sidebar for Metrics and Derived Ground Truth\n",
        "            with st.sidebar:\n",
        "                st.header(\"Metrics and Ground Truth\")\n",
        "                st.write(\"**Automatically Derived Ground Truth:**\")\n",
        "                st.write(ground_truth)\n",
        "\n",
        "                # Metrics Calculation\n",
        "                mrr, ndcg, recall = calculate_metrics(similarity_matrix_rescaled, ground_truth, top_k=10)\n",
        "                st.metric(label=\"Mean Reciprocal Rank (MRR)\", value=f\"{mrr:.4f}\")\n",
        "                st.metric(label=\"Normalized Discounted Cumulative Gain (NDCG)\", value=f\"{ndcg:.4f}\")\n",
        "                st.metric(label=\"Recall\", value=f\"{recall:.4f}\")\n",
        "\n",
        "            # Information Retrieval\n",
        "            ranked_indices = np.argsort(similarity_matrix_rescaled[0])[::-1]\n",
        "            top_k = 3  # Increased top_k for better recall\n",
        "            top_candidates = [candidates[idx] for idx in ranked_indices[:top_k]]\n",
        "\n",
        "            st.write(\"Top 3 Relevant Content:\")\n",
        "            for idx, candidate in enumerate(top_candidates):\n",
        "                truncated_candidate = truncate_text(candidate, max_length=250)\n",
        "                score = similarity_matrix_rescaled[0][ranked_indices[idx]]\n",
        "                st.write(f\"Rank {idx + 1}: {truncated_candidate} (Score: {score:.2f})\")\n",
        "\n",
        "            if st.button(\"Summarization\"):\n",
        "                # Summarization Logic\n",
        "                from groq import Groq\n",
        "                client = Groq(api_key='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')\n",
        "\n",
        "                prompt = \"\"\"\n",
        "                You are a helpful and concise assistant that follows the ReAct pattern step-by-step.\n",
        "                Your task is to summarize the provided content into a short, context-rich response without adding external information.\n",
        "\n",
        "\n",
        "\n",
        "                For every task:\n",
        "                1. Thought: Summarize the content and context, focusing on sessions, timing, and patient progress.\n",
        "                2. Action: Extract key details such as:\n",
        "                  - Document Type (if available)\n",
        "                  - Heading (if available)\n",
        "                  - Date of birth (if available)\n",
        "                  - Physician name (if available)\n",
        "                  - Evaluation date (or relevant dates, e.g., sessions completed or missed)\n",
        "                  - Motor strength (if mentioned)\n",
        "                  - Session details (number of sessions attended, missed, or planned)\n",
        "                  - ICD/CPT codes (if provided)\n",
        "                  - Physical therapy plan (including number of sessions completed, missed, or planned)\n",
        "                3. Observation: Briefly describe the patient's current status, progress, and next steps. If any sessions are mentioned, include them in the description, such as how many sessions have been attended, missed, or planned. Highlight any changes in treatment or future therapy plans.\n",
        "                4. Answer: Provide a concise summary by considering the following from Thought, Action, and Observation:\n",
        "                   \"On [evaluation date], [Document Type], attended by [physician]. The patient [condition/progress]. The patient has completed [number] sessions and missed [number] sessions due to [reason]. [Next steps].\n",
        "                \"\"\"\n",
        "\n",
        "                input_text = \"\\n\".join(top_candidates)\n",
        "\n",
        "                # Sending the request to generate a summary\n",
        "                chat_completion = client.chat.completions.create(\n",
        "                    messages=[\n",
        "                        {\"role\": \"system\", \"content\": prompt},\n",
        "                        {\"role\": \"user\", \"content\": input_text}\n",
        "                    ],\n",
        "                    model=\"llama3-8b-8192\",\n",
        "                    temperature=0.3,\n",
        "                    max_tokens=1000,  # Limit the summary length\n",
        "                    top_p=1,\n",
        "                    stop=None,\n",
        "                    stream=False,\n",
        "                )\n",
        "\n",
        "                # Getting and displaying the summary\n",
        "                summary = chat_completion.choices[0].message.content.strip()\n",
        "\n",
        "                st.write(\"Summary of Top Relevant Content:\")\n",
        "                st.write(summary)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sTjhZOzGETeo",
        "outputId": "02f17bec-0f0d-433f-b05f-3725a448002d"
      },
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overwriting streamlit_app.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Configuring Ngork**"
      ],
      "metadata": {
        "id": "eEgSmlb8FOPA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!ngrok config add-authtoken 2pNSREBSxUcY9bKMdBDQ2GVAi06_6eQPJKfwjhpGstZSAw6rP"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sjj8w4crFZJx",
        "outputId": "b154f394-fbbd-4f4f-f955-2e831243c3fc"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Authtoken saved to configuration file: /root/.config/ngrok/ngrok.yml\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Creating tunnel to load the Streamlit from colab directly**"
      ],
      "metadata": {
        "id": "EpdsfM5CFhym"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pyngrok import ngrok\n",
        "import subprocess\n",
        "\n",
        "# Kill existing Ngrok tunnels if any\n",
        "tunnels = ngrok.get_tunnels()\n",
        "if tunnels:\n",
        "    ngrok.kill()\n",
        "\n",
        "# Start Streamlit in the background\n",
        "process = subprocess.Popen([\"streamlit\", \"run\", \"streamlit_app.py\", \"--server.port\", \"8501\"])\n",
        "\n",
        "# Expose Streamlit app using Ngrok\n",
        "public_url = ngrok.connect(\"8501\")  # Use the correct port here\n",
        "print(f\"Streamlit app is live at: {public_url}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wdUy2DxQEYfO",
        "outputId": "8e2021fa-0e74-4b71-daa4-9b2571fe7361"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Streamlit app is live at: NgrokTunnel: \"https://6279-34-73-220-170.ngrok-free.app\" -> \"http://localhost:8501\"\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "J3ENJCOsRFPh"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}